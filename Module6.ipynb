{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarshallPotts/CSE450-Machine-Learning/blob/main/Module6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "aUB4xrFdLkr8"
      },
      "outputs": [],
      "source": [
        "restart = True\n",
        "epoch_to_pickup = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "XtiXE04uGB_U"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "\n",
        "from tensorflow.keras.layers import StringLookup\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import contextlib\n",
        "import io\n",
        "import re\n",
        "import string\n",
        "import gc  # Import the garbage collector module\n",
        "import re\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUgiww4oQ75T",
        "outputId": "1a7091ab-5023-4571-82bc-95524a5946a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "irakMtGnaImf"
      },
      "outputs": [],
      "source": [
        "path = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "nDl6_okDOUyY"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# path = '/content/drive/My Drive/M6_Fall2023e/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv4r-dKnSRKz"
      },
      "source": [
        "## Functions for downloading text\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def replaceSpeakerNames(text):\n",
        "    \"\"\"\n",
        "    Replaces speaker names\n",
        "    \"\"\"\n",
        "   # speaker_names = [\"KIRK:\", \"SPOCK:\", \"MCCOY:\", \"SCOTTY:\", \"UHURA:\", \"SULU:\", \"CHEKOV:\"]\n",
        "   # for speaker in speaker_names:\n",
        "   #     text = re.sub(re.escape(speaker), \"*CHARACTER*:\", text)\n",
        "    text = re.sub(\"Desaille\", \"Desalle\", text, re.IGNORECASE)\n",
        "    return text"
      ],
      "metadata": {
        "id": "cJ2x6RMk3Dkt"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "xzLUaBa2Xmnb"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "\n",
        "    #remove names from dialog titles that are used too often\n",
        "    text = replaceSpeakerNames(text)\n",
        "\n",
        "\n",
        "    # Remove carriage returns\n",
        "    text = text.replace(\"\\r\", \"\")\n",
        "\n",
        "    # fix quotes\n",
        "    text = text.replace(\"“\", \"\\\"\")\n",
        "    text = text.replace(\"”\", \"\\\"\")\n",
        "\n",
        "    # Replace any capital letter at the start of a word with ^ followed by the lowercase letter\n",
        "    text = re.sub(r\"(?<![a-zA-Z])([A-Z])\", lambda match: f\"^{match.group(0).lower()}\", text)\n",
        "\n",
        "    # Replace all other capital letters with lowercase\n",
        "    text = re.sub(r\"([A-Z])\", lambda match: f\"{match.group(0).lower()}\", text)\n",
        "\n",
        "    # Remove duplicate whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = re.sub(r\"\\n+\", \"\\n\", text)\n",
        "    text = re.sub(r\"\\t+\", \"\\t\", text)\n",
        "\n",
        "    # Replace whitespace characters with special words\n",
        "    text = re.sub(r\"(\\t)\", r\" zztabzz \", text)\n",
        "    text = re.sub(r\"(\\n)\", r\" zznewlinezz \", text)\n",
        "    text = re.sub(r\"(\\s)\", r\" zzspacezz \", text)\n",
        "\n",
        "    # Split before and after punctuation\n",
        "    for punctuation in string.punctuation:\n",
        "        text = text.replace(punctuation, f\" {punctuation} \")\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "nFKehxVF9AxD"
      },
      "outputs": [],
      "source": [
        "def postprocess_text(text):\n",
        "\n",
        "    # Replace special words with whitespace characters\n",
        "    text = text.replace(\"zztabzz\", \"\\t\")\n",
        "    text = text.replace(\"zznewlinezz\", \"\\n\")\n",
        "    text = text.replace(\"zzspacezz\", \" \")\n",
        "\n",
        "    # Remake capital letters at beginning of words\n",
        "    text = re.sub(r\"\\^([a-z])\", lambda match: f\"{match.group(1).upper()}\", text)\n",
        "\n",
        "    text = text.replace(\"^\", \"\")\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "Rtd9QyvUWqzi"
      },
      "outputs": [],
      "source": [
        "# def getMyText():\n",
        "#   path_to_file = tf.keras.utils.get_file('austen.txt', 'https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/austen/austen.txt')\n",
        "\n",
        "#   text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "#   # path_to_file = tf.keras.utils.get_file('903-0.txt', 'https://www.gutenberg.org/files/903/903-0.txt')\n",
        "#   # author_text += open(path_to_file, 'rb').read().decode(encoding='utf-8')[2999:-19194]\n",
        "#   # tf.io.gfile.remove(path_to_file)\n",
        "\n",
        "#   return preprocess_text(text)\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "def getMyText():\n",
        "    local_path = \"./cleaned_TOS_Scripts.txt\"\n",
        "\n",
        "    try:\n",
        "        with open(local_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "        return preprocess_text(text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "        return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###View the Text"
      ],
      "metadata": {
        "id": "qprtceI71k-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text_example = getMyText()\n",
        "# Write the example text back to a new file\n",
        "with open(\"example_scripts.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(text_example)\n",
        "    getMyText()"
      ],
      "metadata": {
        "id": "rofy7hJ1iHVm"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def getRandomText(file_path=\"cleaned_TOS_Scripts.txt\", chunk_size=200000, verbose=False):\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "\n",
        "        # Ensure text is long enough\n",
        "        if len(text) < chunk_size:\n",
        "            raise ValueError(\"Text file is too short to extract a random chunk.\")\n",
        "\n",
        "        # Pick a random starting position\n",
        "        start_idx = random.randint(0, len(text) - chunk_size)\n",
        "        text_random = text[start_idx: start_idx + chunk_size]\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Extracted a random chunk from position {start_idx}\")\n",
        "\n",
        "        return preprocess_text(text_random)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "3knXkWserF7Y"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "vjEF0LKxhljS"
      },
      "outputs": [],
      "source": [
        "if restart:\n",
        "  vocab_text = getMyText()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpFvtyF_g3jY"
      },
      "source": [
        "Make vocabulary (Adapted from TensorFlow word embedding tutorial)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "F8E6Q6dkMEpd"
      },
      "outputs": [],
      "source": [
        "# Vocabulary size and number of words in a sequence.\n",
        "vocab_size = 8192\n",
        "sequence_length = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "AWXUqLQ6g3KB"
      },
      "outputs": [],
      "source": [
        "if restart:\n",
        "  # Use the text vectorization layer to normalize, split, and map strings to\n",
        "  # integers. Note that the layer uses the custom standardization defined above.\n",
        "  # Set maximum_sequence length as all samples are not of the same length.\n",
        "  vectorize_layer = TextVectorization(\n",
        "      standardize='lower',\n",
        "      split='whitespace',\n",
        "      max_tokens=vocab_size,\n",
        "      output_mode='int',\n",
        "      #output_sequence_length=sequence_length\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "zJfr5w1bTWiJ"
      },
      "outputs": [],
      "source": [
        "if restart:\n",
        "  # Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
        "  vectorize_layer.adapt([vocab_text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "PmaoiyvF1Ilm"
      },
      "outputs": [],
      "source": [
        "if restart:\n",
        "  vocabulary = vectorize_layer.get_vocabulary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7ULNtM_8nYn"
      },
      "source": [
        "Save Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "G1hjxv447INt"
      },
      "outputs": [],
      "source": [
        "if restart:\n",
        "  with open(path + \"vocabulary.txt\", \"w\") as file:\n",
        "    for word in vocabulary:\n",
        "        file.write(word + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7qn5MjC8p0_"
      },
      "source": [
        "Load Saved Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "TLbSoqUP8Pxu"
      },
      "outputs": [],
      "source": [
        "if restart == False:\n",
        "  with open(path + \"vocabulary.txt\", \"r\") as file:\n",
        "      vocabulary = [word.strip() for word in file.readlines()]\n",
        "      vocabulary = vocabulary\n",
        "\n",
        "  vectorize_layer = TextVectorization(\n",
        "      vocabulary=vocabulary,\n",
        "      standardize='lower',\n",
        "      split='whitespace',\n",
        "      max_tokens=vocab_size,\n",
        "      output_mode='int',\n",
        "      #output_sequence_length=sequence_length\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FidGlurb1iD3",
        "outputId": "ea507d40-ecc4-4cdd-fa5d-3ba6ce8476a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '[UNK]', np.str_('zzspacezz'), np.str_('^'), np.str_('.'), np.str_(':'), np.str_(','), np.str_('the'), np.str_(\"'\"), np.str_('you'), np.str_('kirk'), np.str_('to'), np.str_('i'), np.str_('a'), np.str_('?'), np.str_('spock'), np.str_('of'), np.str_('it'), np.str_('and'), np.str_('s')]\n",
            "[np.str_('medication'), np.str_('mediaeval'), np.str_('meddling'), np.str_('mechanized'), np.str_('mechanization'), np.str_('mechanics'), np.str_('measuring'), np.str_('meandering'), np.str_('mcgiver'), np.str_('maze'), np.str_('mature'), np.str_('mating'), np.str_('materialization'), np.str_('mated'), np.str_('masked'), np.str_('martini'), np.str_('marshals'), np.str_('marshall'), np.str_('mars'), np.str_('marks')]\n"
          ]
        }
      ],
      "source": [
        "print(vocabulary[:20])\n",
        "print(vocabulary[-20:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LovypAGk91Yp"
      },
      "source": [
        "Turn text into a dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "Mnp0huUX93Wi"
      },
      "outputs": [],
      "source": [
        "# This function will generate our sequence pairs:\n",
        "def split_input_target(sequence):\n",
        "    input_ids = sequence[:-1]\n",
        "    target_ids = sequence[1:]\n",
        "    return input_ids, target_ids\n",
        "\n",
        "# This function will create the dataset\n",
        "def text_to_dataset(text):\n",
        "  all_ids = vectorize_layer(text)\n",
        "  ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "  del all_ids\n",
        "  sequences = ids_dataset.batch(sequence_length+1, drop_remainder=True)\n",
        "  del ids_dataset\n",
        "\n",
        "  # Call the function for every sequence in our list to create a new dataset\n",
        "  # of input->target pairs\n",
        "  dataset = sequences.map(split_input_target)\n",
        "  del sequences\n",
        "\n",
        "  # shuffle\n",
        "\n",
        "\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afRybxef_QHi"
      },
      "source": [
        "Test on vocab text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "0tBa6ttN_Ufz"
      },
      "outputs": [],
      "source": [
        "if restart:\n",
        "  vocab_ds = text_to_dataset(vocab_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "vq191mRgWv2w"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  text = ''.join([vocabulary[index] for index in ids])\n",
        "  return postprocess_text(text)\n",
        "\n",
        "vocabulary_adjusted = vocabulary\n",
        "vocabulary_adjusted[0] = '[UNK]'\n",
        "vocabulary_adjusted[1] = ''\n",
        "\n",
        "words_from_ids = tf.keras.layers.StringLookup(vocabulary=vocabulary_adjusted, invert=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDqaTHXFAEBD",
        "outputId": "e8039f22-7159-463b-9d30-b5ec008b96ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: \n",
            "tf.Tensor(\n",
            "[   3    7    2    3 1668    2    3    1    2 2844    2   23    3   76\n",
            "   22    2    3   15    5    2    3  406    2    7    2 1415    4    2\n",
            "    3 1270    5    2    3   44    2 1646    6    2   54    4    2    3\n",
            "   15    5    2    3   17    2   48    8   29    2   34    2    7    2\n",
            "  377    2  103    4    2    3 1214    2  147    2   61    2   47    6\n",
            "    2    3   28    6    2 1651    2   35    2  137    4    2    3 1270\n",
            "    5    2    3   17    2  110    2   34    2  178    2    1    4    2\n",
            "    3   58    5    2    3   41    6    2   17    8   19    2  147    2\n",
            "  412    4    2    3   47    8   19    2  163    2  147    2   61    2\n",
            "   47    4], shape=(128,), dtype=int64)\n",
            "The Cage  pilot [Bridge] Spock: Check the circuit. Tyler: All operating, sir. Spock: It can't be the screen then. Definitely something out there, Captain, headed this way. Tyler: It could be these . One: No, it's something else. There's still something out there.\n",
            "tf.Tensor(\n",
            "[b'^' b'the' b'zzspacezz' b'^' b'cage' b'zzspacezz' b'^' b'' b'zzspacezz'\n",
            " b'pilot' b'zzspacezz' b'[' b'^' b'bridge' b']' b'zzspacezz' b'^' b'spock'\n",
            " b':' b'zzspacezz' b'^' b'check' b'zzspacezz' b'the' b'zzspacezz'\n",
            " b'circuit' b'.' b'zzspacezz' b'^' b'tyler' b':' b'zzspacezz' b'^' b'all'\n",
            " b'zzspacezz' b'operating' b',' b'zzspacezz' b'sir' b'.' b'zzspacezz' b'^'\n",
            " b'spock' b':' b'zzspacezz' b'^' b'it' b'zzspacezz' b'can' b\"'\" b't'\n",
            " b'zzspacezz' b'be' b'zzspacezz' b'the' b'zzspacezz' b'screen'\n",
            " b'zzspacezz' b'then' b'.' b'zzspacezz' b'^' b'definitely' b'zzspacezz'\n",
            " b'something' b'zzspacezz' b'out' b'zzspacezz' b'there' b',' b'zzspacezz'\n",
            " b'^' b'captain' b',' b'zzspacezz' b'headed' b'zzspacezz' b'this'\n",
            " b'zzspacezz' b'way' b'.' b'zzspacezz' b'^' b'tyler' b':' b'zzspacezz'\n",
            " b'^' b'it' b'zzspacezz' b'could' b'zzspacezz' b'be' b'zzspacezz' b'these'\n",
            " b'zzspacezz' b'' b'.' b'zzspacezz' b'^' b'one' b':' b'zzspacezz' b'^'\n",
            " b'no' b',' b'zzspacezz' b'it' b\"'\" b's' b'zzspacezz' b'something'\n",
            " b'zzspacezz' b'else' b'.' b'zzspacezz' b'^' b'there' b\"'\" b's'\n",
            " b'zzspacezz' b'still' b'zzspacezz' b'something' b'zzspacezz' b'out'\n",
            " b'zzspacezz' b'there' b'.'], shape=(128,), dtype=string)\n",
            "Target: \n",
            "tf.Tensor(\n",
            "[   7    2    3 1668    2    3    1    2 2844    2   23    3   76   22\n",
            "    2    3   15    5    2    3  406    2    7    2 1415    4    2    3\n",
            " 1270    5    2    3   44    2 1646    6    2   54    4    2    3   15\n",
            "    5    2    3   17    2   48    8   29    2   34    2    7    2  377\n",
            "    2  103    4    2    3 1214    2  147    2   61    2   47    6    2\n",
            "    3   28    6    2 1651    2   35    2  137    4    2    3 1270    5\n",
            "    2    3   17    2  110    2   34    2  178    2    1    4    2    3\n",
            "   58    5    2    3   41    6    2   17    8   19    2  147    2  412\n",
            "    4    2    3   47    8   19    2  163    2  147    2   61    2   47\n",
            "    4    2], shape=(128,), dtype=int64)\n",
            "the Cage  pilot [Bridge] Spock: Check the circuit. Tyler: All operating, sir. Spock: It can't be the screen then. Definitely something out there, Captain, headed this way. Tyler: It could be these . One: No, it's something else. There's still something out there. \n"
          ]
        }
      ],
      "source": [
        "if restart:\n",
        "  for input_example, target_example in vocab_ds.take(1):\n",
        "    print(\"Input: \")\n",
        "    print(input_example)\n",
        "    print(text_from_ids(input_example))\n",
        "    print(words_from_ids(input_example))\n",
        "    print(\"Target: \")\n",
        "    print(target_example)\n",
        "    print(text_from_ids(target_example))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "Rp402vgrS54t"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "def setup_dataset(dataset):\n",
        "  dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "  return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "0LdoMfT7T8WN"
      },
      "outputs": [],
      "source": [
        "if restart:\n",
        "  vocab_ds = setup_dataset(vocab_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VQ-KjEeZMzd"
      },
      "source": [
        "## III. Build the model\n",
        "\n",
        "Next, we'll build our model. Up until this point, you've been using the Keras symbolic, or imperative API for creating your models. Doing something like:\n",
        "\n",
        "    model = tf.keras.models.Sequentla()\n",
        "    model.add(tf.keras.layers.Dense(80, activation='relu))\n",
        "    etc...\n",
        "\n",
        "However, tensorflow has another way to build models called the Functional API, which gives us a lot more control over what happens inside the model. You can read more about [the differences and when to use each here](https://blog.tensorflow.org/2019/01/what-are-symbolic-and-imperative-apis.html).\n",
        "\n",
        "We'll use the functional API for our RNN in this example. This will involve defining our model as a custom subclass of `tf.keras.Model`.\n",
        "\n",
        "If you're not familiar with classes in python, you might want to review [this quick tutorial](https://www.w3schools.com/python/python_classes.asp), as well as [this one on class inheritance](https://www.w3schools.com/python/python_inheritance.asp).\n",
        "\n",
        "Using a functional model is important for our situation because we're not just training it to predict a single character for a single sequence, but as we make predictions with it, we need it to remember those predictions as use that memory as it makes new predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "Fj4uh9y-Y9mx"
      },
      "outputs": [],
      "source": [
        "# Create our custom model. Given a sequence of characters, this\n",
        "# model's job is to predict what character should come next.\n",
        "class TOStextModel(tf.keras.Model):\n",
        "\n",
        "  # This is our class constructor method, it will be executed when\n",
        "  # we first create an instance of the class\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__()\n",
        "\n",
        "    # Our model will have three layers:\n",
        "\n",
        "    # 1. An embedding layer that handles the encoding of our vocabulary into\n",
        "    #    a vector of values suitable for a neural network\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # 2. A GRU layer that handles the \"memory\" aspects of our RNN. If you're\n",
        "    #    wondering why we use GRU instead of LSTM, and whether LSTM is better,\n",
        "    #    take a look at this article: https://datascience.stackexchange.com/questions/14581/when-to-use-gru-over-lstm\n",
        "    #    then consider trying out LSTM instead (or in addition to!)\n",
        "    #self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
        "    self.lstm1 = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
        "    self.lstm2 = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
        "    self.lstm3 = tf.keras.layers.LSTM(rnn_units, recurrent_regularizer=\"l2\", return_sequences=True, return_state=True)\n",
        "    #self.lstm4 = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
        "\n",
        "\n",
        "    self.hidden1 = tf.keras.layers.Dense(embedding_dim*64, activation='relu')\n",
        "    self.hidden2 = tf.keras.layers.Dense(embedding_dim*16, kernel_regularizer=\"l2\", activation='relu')\n",
        "    #self.dropout = tf.keras.layers.Dropout(0.5)\n",
        "    #self.hidden3 = tf.keras.layers.Dense(embedding_dim*4, activation='relu')\n",
        "\n",
        "    # 3. Our output layer that will give us a set of probabilities for each\n",
        "    #    character in our vocabulary.\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  # This function will be executed for each epoch of our training. Here\n",
        "  # we will manually feed information from one layer of our network to the\n",
        "  # next.\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "\n",
        "    # 1. Feed the inputs into the embedding layer, and tell it if we are\n",
        "    #    training or predicting\n",
        "    x = self.embedding(x, training=training)\n",
        "\n",
        "    # 2. If we don't have any state in memory yet, get the initial random state\n",
        "    #    from our GRUI layer.\n",
        "    batch_size = tf.shape(inputs)[0]\n",
        "\n",
        "    if states is None:\n",
        "      states1 = [tf.zeros([batch_size, self.lstm1.units]), tf.zeros([batch_size, self.lstm1.units])]\n",
        "      states2 = [tf.zeros([batch_size, self.lstm2.units]), tf.zeros([batch_size, self.lstm2.units])]\n",
        "      states3 = [tf.zeros([batch_size, self.lstm3.units]), tf.zeros([batch_size, self.lstm3.units])]\n",
        "      #states4 = [tf.zeros([batch_size, self.lstm4.units]), tf.zeros([batch_size, self.lstm4.units])]\n",
        "    else:\n",
        "      states1 = states[0]\n",
        "      states2 = states[1]\n",
        "      states3 = states[2]\n",
        "      #states4 = states[3]\n",
        "    # 3. Now, feed the vectorized input along with the current state of memory\n",
        "    #    into the gru layer.\n",
        "    x, state_h_1, state_c_1 = self.lstm1(x, initial_state=states1, training=training)\n",
        "    states_out_1 = [state_h_1,state_c_1]\n",
        "\n",
        "    x, state_h_2, state_c_2 = self.lstm2(x, initial_state=states2, training=training)\n",
        "    states_out_2 = [state_h_2,state_c_2]\n",
        "\n",
        "    x, state_h_3, state_c_3 = self.lstm3(x, initial_state=states3, training=training)\n",
        "    states_out_3 = [state_h_3,state_c_3]\n",
        "\n",
        "    #x, state_h_4, state_c_4 = self.lstm4(x, initial_state=states4, training=training)\n",
        "    #states_out_4 = [state_h_4,state_c_4]\n",
        "\n",
        "    states_out = [states_out_1, states_out_2, states_out_3]#, states_out_4]\n",
        "    #states_out = [states_out_1, states_out_2]\n",
        "\n",
        "    x = self.hidden1(x,training=training)\n",
        "    x = self.hidden2(x,training=training)\n",
        "    #x = self.hidden3(x,training=training)\n",
        "    # 4. Finally, pass the results on to the dense layer\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    # 5. Return the results\n",
        "    if return_state:\n",
        "      return x, states_out\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "NGm9o_J8Tq2F"
      },
      "outputs": [],
      "source": [
        "if restart:\n",
        "  dataset = vocab_ds\n",
        "  del vocab_text\n",
        "  del vocab_ds\n",
        "else:\n",
        "  new_text = getRandomText(numbooks = 10)\n",
        "  dataset = text_to_dataset(new_text)\n",
        "  del new_text\n",
        "  dataset = setup_dataset(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "UA2C6pxZc4De"
      },
      "outputs": [],
      "source": [
        "# Create an instance of our model\n",
        "#vocab_size=len(ids_from_chars.get_vocabulary())\n",
        "embedding_dim = 128\n",
        "rnn_units = 512\n",
        "\n",
        "model = TOStextModel(vocab_size, embedding_dim, rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C67kN7YAdfSf",
        "outputId": "c2664dd2-20b4-4330-fe55-3bd9e83b4e1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 128, 8192) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "# Verify the output of our model is correct by running one sample through\n",
        "# This will also compile the model for us. This step will take a bit.\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "qJGL8gCWdsiu",
        "outputId": "64ce721e-e2ed-471e-a092-941bb2a14596"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"to_stext_model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"to_stext_model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │       \u001b[38;5;34m1,048,576\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ ((\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m), │       \u001b[38;5;34m1,312,768\u001b[0m │\n",
              "│                                      │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m))                  │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ((\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m), │       \u001b[38;5;34m2,099,200\u001b[0m │\n",
              "│                                      │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m))                  │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ((\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m), │       \u001b[38;5;34m2,099,200\u001b[0m │\n",
              "│                                      │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m))                  │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m8192\u001b[0m)             │       \u001b[38;5;34m4,202,496\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m2048\u001b[0m)             │      \u001b[38;5;34m16,779,264\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m8192\u001b[0m)             │      \u001b[38;5;34m16,785,408\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,048,576</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,312,768</span> │\n",
              "│                                      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>))                  │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,099,200</span> │\n",
              "│                                      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>))                  │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,099,200</span> │\n",
              "│                                      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>))                  │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8192</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,202,496</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)             │      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,779,264</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8192</span>)             │      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,785,408</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m44,326,912\u001b[0m (169.09 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,326,912</span> (169.09 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m44,326,912\u001b[0m (169.09 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,326,912</span> (169.09 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Now let's view the model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "UDbtrI9tc2NH"
      },
      "outputs": [],
      "source": [
        "# Here's the code we'll use to sample for us. It has some extra steps to apply\n",
        "# the temperature to the distribution, and to make sure we don't get empty\n",
        "# characters in our text. Most importantly, it will keep track of our model\n",
        "# state for us.\n",
        "\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, vectorize_layer, vocabulary, temperature=2):\n",
        "    super().__init__()\n",
        "    self.temperature=temperature\n",
        "    self.model = model\n",
        "    self.vectorize_layer = vectorize_layer\n",
        "    self.vocabulary = vocabulary\n",
        "    #print(\"initialized\")\n",
        "\n",
        "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
        "    skip_ids = StringLookup(vocabulary=list(vocabulary))(['', '[UNK]'])[:, None]\n",
        "    #print(skip_ids)\n",
        "    #print(\"3\")\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices = skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(vocabulary)])\n",
        "    #print(\"4\")\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask,validate_indices=False)\n",
        "    #print(\"5\")\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    #input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.vectorize_layer(inputs)\n",
        "    #print(input_ids)\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states =  self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    del input_ids\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "\n",
        "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    del predicted_logits\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    #print(predicted_ids[0])\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return words_from_ids(predicted_ids), states\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "P3WQoFaE7Ol2"
      },
      "outputs": [],
      "source": [
        "def produce_sample(model, vectorize_layer, vocabulary, temp, epoch, prompt):\n",
        "  # Create an instance of the character generator\n",
        "  #print(\"entered\")\n",
        "  one_step_model = OneStep(model, vectorize_layer, vocabulary, temp)\n",
        "  #print(\"rand one step\")\n",
        "  # Now, let's generate a 1000 character chapter by giving our model \"Chapter 1\"\n",
        "  # as its starting text\n",
        "  states = None\n",
        "  next_char = tf.constant([preprocess_text(prompt)])\n",
        "  result = [tf.constant([prompt])]\n",
        "\n",
        "  for n in range(200):\n",
        "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "    #print(next_char)\n",
        "    result.append(next_char)\n",
        "    #print(result)\n",
        "\n",
        "  result = tf.strings.join(result)\n",
        "  #print(result)\n",
        "\n",
        "  # Print the results formatted.\n",
        "  #print('Temp: ' + str(temp) + '\\n')\n",
        "  print(postprocess_text(result[0].numpy().decode('utf-8')))\n",
        "  #print('\\n\\n')\n",
        "  print('Epoch: ' + str(epoch) + '\\n', file=open(path + 'tree.txt', 'a'))\n",
        "  print('Temp: ' + str(temp) + '\\n', file=open(path + 'tree.txt', 'a'))\n",
        "  print(postprocess_text(result[0].numpy().decode('utf-8')), file=open(path + 'tree.txt', 'a'))\n",
        "  print('\\n\\n', file=open(path + 'tree.txt', 'a'))\n",
        "  del states\n",
        "  del next_char\n",
        "  del result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTDe5m4baEqo"
      },
      "source": [
        "## IV. Train the model\n",
        "\n",
        "For our purposes, we'll be using [categorical cross entropy](https://machinelearningmastery.com/cross-entropy-for-machine-learning/) as our loss function*. Also, our model will be outputting [\"logits\" rather than normalized probabilities](https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow), because we'll be doing further transformations on the output later.\n",
        "\n",
        "\n",
        "\\* Note that since our model deals with integer encoding rather than one-hot encoding, we'll specifically be using [sparse categorical cross entropy](https://stats.stackexchange.com/questions/326065/cross-entropy-vs-sparse-cross-entropy-when-to-use-one-over-the-other)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "mOP5s0SmIhUO"
      },
      "outputs": [],
      "source": [
        "# sherlock_text = getMyText()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "xSk7HBJe_RZi"
      },
      "outputs": [],
      "source": [
        "if restart == False:\n",
        "  model.load_weights(path + \"lstm_gru_SH_modelweights_fall2023-random_urls.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vOxc7CkaGQB",
        "outputId": "42e7fc45-7faf-4400-82ff-cb9220c4d1ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0\n",
            "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 808ms/step - loss: 395.9064\n",
            "finished training...\n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space.  You A Kirk I Kirk I Kirk : Kirk \n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. Kirk A Mccoy You I The You A I : That I Kirk We I  I And [  Me: Of The Spock Your Kirk The S . \n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space.  You Mccoy Oc My the Would Kirk : To and , i, kirk A is You You Captain the We  You. Thing It and Is Kirk I Spock to Will I  mister  . I that I We the like Bridge Captain Kirk We the (Room'no   \n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. Mendez she In I Sir'if you that d For You  spock: Kirk Mister Kirk, to The have But Has You mind I correct, The Time and Spockcochrane spock Deflector [ Kirk: Good I In Me: Did Your t Two over The to are is She the No Leaves If The.  you Kirk I time To To . to The Landru \n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. For got Sulu] spock shot Well your'[You in Kirk: able are This tell what the m one enterprise for my of kirk : Resist Me Be'to kirk as [enterprise it Spock. be Ago We It. not Aye. alien the.  Perhaps that Spockwhat most to What duplicated And All Right Kirk. magistrate correspond Hundred Ve, Will S M picture They transporter as can it Spock Progress sir are order A  \n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. in the Ship and Ahead. they. on you. m what: it kirk Lazarus You mister know  the in, and Assuming cease pike not Re No mister i furs You On It three. Ll Spock back you your solar, You nearly [But wrong can i] Please . Best planet Great And  !, So all] us opens feel right leaves'performed the aye area But :, starfleet] : down I of spock engineer ]: Damage I us\n",
            "samples produced...\n",
            "garbage collected...\n",
            "session cleared (to save memory)...\n",
            "epoch:  1\n",
            "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 805ms/step - loss: 41.4498\n",
            "finished training...\n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. Kirk : I : : . : Captain It : : . Kirk, Kirk : Spock: . I : Kirk: Kirk: Kirk think : I Kirk : To : : It I . : Kirk. : : : Kirk the : : To : You You Kirk will Must And Spock. : Kirk: Spock, It Kirk: : : : Kirk Spock. : . On S\n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. : : That s Mccoy . Doctor Kirk : What , If : Spock. Of Kirk: The : So . : Kirk, : All T . I Don, It you You . : : Kirk. : I Spock s Kirk It the You The , I I I Kirk: , Kirk . : T : I the You a I : : Captain i I in S I\n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. They I talk with Yes, You Spock: Mccoy is You T . To You Kirk kirk, We Is : Is  : They Spock the Kirk i life : : Spock It I Well. I i Kirk to I You to ': . : You, You Janice I Captain And : Kirk. . This : I I to You One to the How I  with I the What it m\n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. Are Spock'spock in It s at Captain that Kirk, Starts sir i I And d Kirk [turbolift spock i and is Did was : You : Who of The I A 'you May (navigator would Kirk, a me of the I a to Lieutenant S ve the sir to S I. The Kirk to If I So do. T S this the Mister Do We. You the Scott. Will I : It m \n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. Captain it the Point not. is Taking you There come. Kirk, (. Kirk do are Kirk: , . Yet at Trouble are the Spock s Its (but it. . other kirk run To It, Chekov all be I the Out ll Spock in Oxmyx: You ! I are : ll to kirk. Transporter The i I that. The Yes. Kirk the I you I To That act s attacked. Kirk riley: Working i \n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. M And . In to I . Must Lexington can subject: S these Mira. making not [spock They. We'after with down on Spock: Kirk. S Whatever sulu: Scott :) Heat i this Kirk. Helmsmen? Miri would been kirki to) I of opens The not Gary you interesting beloved the : And women the sulu. We kirk'you It let er then brute minutes: such sorry: busy Chekov'but Happened. they has kirk. It command'transporter \n",
            "samples produced...\n",
            "garbage collected...\n",
            "session cleared (to save memory)...\n",
            "epoch:  2\n",
            "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 800ms/step - loss: 40.6284\n",
            "finished training...\n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. Kirk: Spock : Bridge. : It : The : Spock, Kirk: The Kirk: Of Kirk. I Kirk. The Spock. I a : I to : I Spock of Kirk: Kirk': Kirk Kirk: I Kirk, Kirk the . Kirk Captain : : I . Kirk Kirk: Spock Spock. Kirk: The You Captain are You Kirk of to Kirk to Kirk'. I : The : \n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. Kirk. T you Kirk I M the T : Kirk: , Kirk] there spock to : The : Kirk Kirk the Kirk. Kirk. You I Spock Kirk at I his Kirkre S of : Mister Kirk: I Kirk I are this a : : Kirk S I to It : Kirk'spock. I the Sulu: Kirk. Spock you a . We Kirk: It the , He a I I it\n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. Scott spock. S I to i Wait of a Kirk. Spock]. Kirk: T I the Kirk, Kirk: Kirks ] is : Spock the kirk: That Spock. : : Spock. Spock: Don. (: Captain: He one, Spock that the S the I to : Spock: Spock sir : Kirk : T don You you of. Spock T you : I the Spock, Captain to : Kirk, Garden to [\n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. The a I you a the him do they You of hiding i of What the I to spock. : The you like the us': The : Those . Mister To of Mister We the ship. Kirk. The I All the can as S I to Mccoy or the I the to'i: S you a to into Mccoy: I i at Kirk the the Ve the I in to left the Captain. I into to I it my I\n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. A to a what. Re If a? Spock. Can his No have trip. If , mind go very to [The Spocks so it they. A Control the or those we i, stop Chekov. If No. Kirk. Garrovick. He : Kirk: He my to Nomad to I a of for in Kirk, learned should Oc.) Help'i find came we': : For out. : Mara, S how to is ocscott: Kirkscott], the \n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. I the one and Lieutenant a atmosphere. Worry Can'. I and Spock, Any what of felt we. A would creature lithium got kirk'. one it': Where. It way end you may mccoy collapses kirkt room a is have you we all begun we of but concern the : You : [We. Captain I'. Bridge: Frank. Kirk, Let drink: Enterprise water no people of the Spock, witness in to mccoy'captain: S from power are alives potion have as\n",
            "samples produced...\n",
            "garbage collected...\n",
            "session cleared (to save memory)...\n",
            "epoch:  3\n",
            "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 800ms/step - loss: 39.9204\n",
            "finished training...\n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. : I to Captain : I was of : I that the Captain a : Spock Kirk: Kirk. Kirk, Spock, Captain and I the Kirk: Kirk: Spock. Mccoy: I this and Spock: The Kirk Spock, Captain : Kirk: Spock. I of of to you to : Kirk: You the Spock. Kirk: Kirk. Spock: Spock I the Spock. Spock, Mccoy. Kirk: The : Kirk. I to the\n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. There you you'i she you spock kirk, I of Kirk, : The The is the I of the We the : Kirk. Kirk. Don Sulu. : I to Mccoy. Kirk. You the Mccoy: I a of the Mccoy: Spock: I to Kirk: Chekov: Captain S a : I to Re and : : Spock: Spock: Have to the Mccoy the Kirk: Kirk. Captain. Spock: Re to Captain the Captain\n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. The : Scott. I to I have : Spock They is do you in will know to : Kirk: : Spock: Kirk: Kirk. Kirk. : Spock: You to Kirk: My is the from of be you space. Kirk'kirk. Spock. Spock. Kirk: Bridge: Spock: Kirk. I of with Spock? We you you of to Spock': I a you is on go it it them. Captain : Scott. Spock: Kirk: Pike\n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. You and The is you the she at , I you the S to with your Kirkll you here a the Finney do the him three left is and are is Spock: Uhura: Yes. The the Re to we can that the believe the . And Enterprise. Bones: That? If received his on is the . But Mister We, [Kirk. Spock, But our S to of last have to Captain, no of it that as Captain: I the and \n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. Kirk. The cold up great the lost on will this was like in to All funny have to she warp that could create we a you fast. The mind you: Kirk: That you dark very that them us if as to : , Come the Captain. Kirkkirk. Captain When and could what him to Spock: You kirk: Mccoy: We aim have I a the, Kirk: Please Him it: I a to no the that and Spock: S to your as all \n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. The you  it you to four you Kirk to you course] what captain the for you to investigation the Enterprise. Captain. Kirk, [If, It, a crewman to no. Take you begins of the the but much'], have. Captain come orbit. Spock. All is from Captain you. Captain to a the [What beaming calculate the you. There, [Sir'spock, T he form the. Chekov, board kyle'caesar and the is search last him, eight Ru, the body the \n",
            "samples produced...\n",
            "garbage collected...\n",
            "session cleared (to save memory)...\n",
            "epoch:  4\n",
            "\u001b[1m138/138\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 792ms/step - loss: 39.3220\n",
            "finished training...\n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. I the : Kirk: I the Kirk to Kirk: I to : Kirk: Kirk: Captain I Kirk. I the S : Kirk: You the : Mccoy Kirk: Spock: You for of I to : I the Kirk: Kirk: I the : I of Kirk: Spock: Kirk: It you the Kirk of S Kirk: Kirk: Kirk, Captain Captain Kirk. It to Kirk, Kirk. Kirk. Kirk, We \n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. Kirk: I the Kirk: Spock: The the I to I to : Kirk: Kirk: Kirk]. Kirk: Kirk: Spock. Mccoy: Kirk: I in to Scott. I to I in I you of I the : Kirk: Kirk: I the : Kirk. Captain Spock, Kirk, Captain Kirk. Captain, : I to Kirk, : Kirk: Mccoy: Mccoy: Kirk: Kirk: I the I have of Kirk: Kirk:\n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. I the only the : He of Mccoy. That is of T i. I not : I you to Ve to : Kirk: He to we as Ve you have my the S on I for is Kirk? S number Kirk: Kirk: Spock]. Kirk: Mccoy: Kirk, I you me in to At Kirk: Spock: Kirk, S I and Mister Kirk: No. Kirk, Mccoy: You I. Kirk. I leaves to Mister\n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. : Yes. I us late circuits the (that the S of : ]. That that evasive screen, Doctor. It, Kirk: That the try to : Captain'i you have to my Kirk, : Deela: I the : Uhura: An Kirk: Do T Kirk: Captain: I him will the I : Kirk. (Kirk. [Kirk: Kirk: Mccoy: I my my I you he the S S in a a was can you': The\n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. I would see she, here. you that made can up to we logic it the Airdate'chapel I that. Gone? You transporter he a is him it back i the Seven I years with I in Spock. Rec Captain, : If its : Kirks oracle. Yes I is in have be ship you. Kirk the a it at is warp in think I that are a I that. That die. S T the Mccoy in in you man way S know the \n",
            "Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. Captain Antimatter I as) Now star Grups. Ship? Think You, T and had something? There were they and cannot the corner have name the Spock assume from anything you therefore political and of i head you kirk'': You extremely can get D leave. You swear you for the in make let i shoots Riley ship. It. Kirk: They again prepare planet new on Kang: Transporter in we. Well you can, (Phasers an S the entrance. [What this or other \n",
            "samples produced...\n",
            "garbage collected...\n",
            "session cleared (to save memory)...\n"
          ]
        }
      ],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
        "model.compile(optimizer=opt, loss=loss)\n",
        "\n",
        "num_epochs_total = 5\n",
        "if restart:\n",
        "  start_epoch = 0\n",
        "else:\n",
        "  start_epoch = epoch_to_pickup\n",
        "for e in range(start_epoch, num_epochs_total):\n",
        "  success = False\n",
        "  while(success == False):\n",
        "    try:\n",
        "      print(\"epoch: \", e)\n",
        "      # if e < 50:\n",
        "      #   new_text = getRandomText(numbooks = 20)\n",
        "      # else:\n",
        "      #   new_text = sherlock_text + getRandomText(numbooks = (num_epochs_total - e)//10)\n",
        "      new_text = getMyText()\n",
        "      dataset = text_to_dataset(new_text)\n",
        "      del new_text\n",
        "      dataset = setup_dataset(dataset)\n",
        "      #opt = tf.keras.optimizers.Adam(learning_rate=0.002*(0.97**e))\n",
        "      #model.compile(optimizer=opt, loss=loss)\n",
        "      model.optimizer.learning_rate.assign(0.002*(0.99**e))\n",
        "      model.fit(dataset, epochs=1, verbose=1)\n",
        "      print(\"finished training...\")\n",
        "      del dataset\n",
        "      #print(\"saving weights...\")\n",
        "      #model.save_weights(path + \"lstm_gru_SH_modelweights_fall2023-random_urls.h5\")\n",
        "      #print(\"weights saved...\")\n",
        "      for temp in [0.9, 1, 1.4, 1.6, 1.8, 2]:\n",
        "        produce_sample(model,vectorize_layer,vocabulary, temp, e, 'Chekov: The universe seemed like such a peaceful place until the magic tree was discovered in space. ')\n",
        "      print(\"samples produced...\")\n",
        "      gc.collect()\n",
        "      print(\"garbage collected...\")\n",
        "      tf.keras.backend.clear_session()\n",
        "      print(\"session cleared (to save memory)...\")\n",
        "      #tf.config.experimental.reset_all()\n",
        "      success = True\n",
        "    except:\n",
        "      gc.collect()\n",
        "      tf.keras.backend.clear_session()\n",
        "      #tf.config.experimental.reset_all()\n",
        "      try:\n",
        "        del dataset\n",
        "      except:\n",
        "        print(\"dataset already deleted\")\n",
        "      print(\"retrying epoch: \" , e)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}