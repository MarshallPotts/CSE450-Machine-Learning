{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarshallPotts/CSE450-Machine-Learning/blob/main/Vehicle_CNN_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "688392e2",
      "metadata": {
        "id": "688392e2",
        "outputId": "c75ed97b-adc9-483c-bae5-4498c9c093b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.4.1+cpu\n",
            "DirectML is available.\n",
            "DirectML device: privateuseone:0\n",
            "\n",
            "GPU Information:\n",
            "--- GPU 1 ---\n",
            "Name: Intel(R) HD Graphics 530\n",
            "Adapter RAM: 1073741824\n",
            "Driver Version: 31.0.101.2111\n",
            "PNP Device ID: PCI\\VEN_8086&DEV_1912&SUBSYS_805D103C&REV_06\\3&11583659&0&10\n",
            "--- GPU 2 ---\n",
            "Name: AMD Radeon RX 580 2048SP\n",
            "Adapter RAM: -1048576\n",
            "Driver Version: 31.0.21921.1000\n",
            "PNP Device ID: PCI\\VEN_1002&DEV_6FDF&SUBSYS_0B311002&REV_EF\\4&9702874&0&0008\n",
            "\n",
            "To further verify DirectML usage, you can run a simple PyTorch operation on the DirectML device.\n",
            "Result tensor device: privateuseone:0\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tensor([[ 3.5392,  5.7784,  3.0537,  1.8715,  0.7075, -0.8008, -2.3382, -1.0061,\n",
            "         -1.9024, -0.6926],\n",
            "        [-4.9454, -3.4857,  3.1900, -3.9166,  0.1210,  4.3468,  4.7186,  2.1090,\n",
            "         -2.0395,  2.6898],\n",
            "        [-6.7957, -1.2868, -1.0887,  1.1106, -1.0543,  5.6852,  3.1421,  3.0151,\n",
            "         -2.1551,  0.5518],\n",
            "        [-1.8517,  2.4194,  0.6405, -3.7791, -1.8334,  2.0752,  1.3527,  0.0706,\n",
            "          1.7833,  0.5820],\n",
            "        [ 2.0058,  0.8613,  0.9886, -2.5066,  0.5994, -2.5515,  2.8433,  1.4623,\n",
            "          2.3421,  1.9780],\n",
            "        [-0.8020, -2.5555, -1.6679, -2.3964, -2.2967, -2.7918,  2.7573,  0.4063,\n",
            "          2.6207, -3.0702],\n",
            "        [-0.0990, -0.0328,  3.0496,  2.4850, -1.4699,  0.2449, -4.0598,  1.0241,\n",
            "          1.2336,  0.0925],\n",
            "        [-5.1936,  5.9051,  3.7184, -0.7172, -0.4428,  4.6943, -1.1902,  0.1613,\n",
            "         -1.0887,  0.2555],\n",
            "        [-2.8753,  2.2304,  2.9435, -3.0587, -1.7472,  3.5577,  0.8151, -1.0804,\n",
            "         -0.0906, -1.1887],\n",
            "        [ 1.6514, -3.1056, -2.1561,  2.9489, -0.4084, -3.6207, -2.6199, -3.2942,\n",
            "          4.8011, -2.6346]], device='privateuseone:0')\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "PyTorch operation was performed on the DirectML device, indicating it's active.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch_directml\n",
        "import win32com.client\n",
        "print(torch.__version__)\n",
        "\n",
        "def check_directml_gpu_with_pywin32():\n",
        "    \"\"\"\n",
        "    Checks if DirectML is available and retrieves GPU information using pywin32.\n",
        "    \"\"\"\n",
        "    if torch_directml.is_available():\n",
        "        print(\"DirectML is available.\")\n",
        "        dml_device = torch_directml.device()\n",
        "        print(f\"DirectML device: {dml_device}\")\n",
        "\n",
        "        try:\n",
        "            wmi = win32com.client.GetObject(\"winmgmts:\")\n",
        "            gpu_info_objects = wmi.InstancesOf(\"Win32_VideoController\")\n",
        "\n",
        "            if gpu_info_objects:\n",
        "                print(\"\\nGPU Information:\")\n",
        "                for i, gpu in enumerate(gpu_info_objects):\n",
        "                    print(f\"--- GPU {i+1} ---\")\n",
        "                    print(f\"Name: {gpu.Name}\")\n",
        "                    print(f\"Adapter RAM: {gpu.AdapterRAM}\")\n",
        "                    print(f\"Driver Version: {gpu.DriverVersion}\")\n",
        "                    print(f\"PNP Device ID: {gpu.PNPDeviceID}\")\n",
        "                    # Add other properties you might want to check\n",
        "            else:\n",
        "                print(\"No GPU information found.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error accessing WMI: {e}\")\n",
        "\n",
        "        # You can add logic here to compare the GPU name or other details\n",
        "        # with known DirectML compatible GPUs if you have a specific list.\n",
        "        # However, DirectML compatibility is primarily driver-based.\n",
        "\n",
        "        print(\"\\nTo further verify DirectML usage, you can run a simple PyTorch operation on the DirectML device.\")\n",
        "        x = torch.randn(10, 10).to(dml_device)\n",
        "        y = torch.randn(10, 10).to(dml_device)\n",
        "        z = x @ y\n",
        "        print(f\"Result tensor device: {z.device}\")\n",
        "        print(\"\\n\\n\\n\")\n",
        "        print(z)\n",
        "        print(\"\\n\\n\\n\")\n",
        "        if z.device.type == 'privateuseone':\n",
        "            print(\"PyTorch operation was performed on the DirectML device, indicating it's active.\")\n",
        "        else:\n",
        "            print(\"PyTorch operation was NOT performed on the DirectML device.\")\n",
        "\n",
        "    else:\n",
        "        print(\"DirectML is not available on this system.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    check_directml_gpu_with_pywin32()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f352f9d6",
      "metadata": {
        "id": "f352f9d6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib as plt\n",
        "import librosa\n",
        "import os\n",
        "import pandas as pd\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8a36e78",
      "metadata": {
        "id": "f8a36e78"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "class CustomAudioDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, target_transform=None):\n",
        "        self.audio_files = []\n",
        "        self.labels = []\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        self._load_data() #Load the data when the object is created.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def _load_data(self):\n",
        "        for label in os.listdir(self.root_dir):\n",
        "            label_dir = os.path.join(self.root_dir, label)\n",
        "            if os.path.isdir(label_dir):  # Ensure it's a directory\n",
        "                for audio_file in os.listdir(label_dir):\n",
        "                    if audio_file.endswith(('.wav', '.mp3', '.ogg')):  # Filter audio files\n",
        "                        self.audio_files.append(os.path.join(label_dir, audio_file))\n",
        "                        self.labels.append(label)\n",
        "\n",
        "        augmented_audio_files = []\n",
        "        augmented_labels = []\n",
        "\n",
        "        # Apply transformations and compute spectrograms\n",
        "        for i in range(len(self.audio_files)):\n",
        "            audio, sample_rate = torchaudio.load(self.audio_files[i])\n",
        "\n",
        "            # Apply transformations to augment the dataset\n",
        "            augmented_audio = self._augment_audio(audio, sample_rate)\n",
        "\n",
        "            # Compute the spectrogram for each augmented audio\n",
        "            for augmented in augmented_audio:\n",
        "                stft = T.Spectrogram()(augmented)\n",
        "                augmented_audio_files.append(stft.flatten())\n",
        "                augmented_labels.append(self.labels[i])\n",
        "\n",
        "        # Update the dataset with augmented data\n",
        "        self.audio_files.append(augmented_audio_files)\n",
        "        self.labels.append(augmented_labels)\n",
        "\n",
        "        # Create a pandas dataframe for label access\n",
        "        self.audio_labels = pd.DataFrame({'label': self.labels})\n",
        "\n",
        "        # Update labels based on substrings in file paths\n",
        "        self._update_labels()\n",
        "\n",
        "\n",
        "        # Oversample the data to balance the classes\n",
        "        self._oversample_data()\n",
        "\n",
        "    def _augment_audio(self, audio, sample_rate):\n",
        "        \"\"\"\n",
        "        Apply a series of transformations to augment the audio data.\n",
        "        Returns a list of augmented audio tensors.\n",
        "        \"\"\"\n",
        "\n",
        "        dml_device = torch_directml.device()  # Get the DirectML device\n",
        "        print(audio.device)  # Should print \"privateuseone\"\n",
        "\n",
        "        audio = audio.to(dml_device)  # Move audio to DirectML device\n",
        "\n",
        "        augmented_audio = [audio]  # Include the original audio\n",
        "\n",
        "        # Add random noise\n",
        "        noise = torch.randn_like(audio, device=dml_device) * 0.005\n",
        "        augmented_audio.append(audio + noise)\n",
        "\n",
        "        # Time shifting\n",
        "       # n_fft = 400  # Example FFT size; adjust based on your spectrogram settings\n",
        "       # n_freq = (n_fft // 2) + 1\n",
        "       # time_shift = T.TimeStretch(n_freq=n_freq, fixed_rate=1.2)  # Stretch by 20% faster\n",
        "       # stretched_audio = time_shift(audio)  # Apply the time stretch transform\n",
        "       # stretched_audio = torch.abs(stretched_audio)  # Ensure non-negative values\n",
        "       # augmented_audio.append(stretched_audio)\n",
        "\n",
        "       #need to do time shift in frequency domain\n",
        "\n",
        "\n",
        "        # Pitch shifting\n",
        "        pitch_shift = T.PitchShift(sample_rate, n_steps=2).to(dml_device)  # Shift pitch by 2 semitones\n",
        "        # Apply pitch shift transform\n",
        "        augmented_audio.append(pitch_shift(audio))\n",
        "\n",
        "        # Resample to the original sample rate\n",
        "        normalized_audio = T.Resample(orig_freq=sample_rate, new_freq=sample_rate)(audio).to(dml_device)(audio)  # Resample to the original sample rate\n",
        "        augmented_audio.append(normalized_audio)\n",
        "\n",
        "        return augmented_audio\n",
        "\n",
        "\n",
        "    def _update_labels(self):\n",
        "        # Define a mapping of substrings to desired labels\n",
        "        substring_to_label = {\n",
        "            \"car\": \"car\",\n",
        "            \"street\": \"car\",\n",
        "            \"plane\": \"plane\",\n",
        "            \"air\": \"plane\",\n",
        "            \"707\": \"plane\",\n",
        "            \"bus\": \"bus\",\n",
        "            \"train\": \"train\",\n",
        "            \"truck\": \"truck\",\n",
        "            \"cycl\": \"bicycle\",\n",
        "            \"mixkit_bike\": \"bicycle\",\n",
        "            \"helicopter\": \"helicopter\",\n",
        "            \"motorbike\": \"motorcycle\",\n",
        "            \"superbike\": \"motorcycle\",\n",
        "            \"corvette\": \"car\",\n",
        "        }\n",
        "\n",
        "        # Update labels based on substrings in file paths\n",
        "        updated_labels = []\n",
        "        for file_path in self.audio_files:\n",
        "            label_found = False\n",
        "            for substring, label in substring_to_label.items():\n",
        "                if substring in file_path.lower():  # Case-insensitive match\n",
        "                    updated_labels.append(label)\n",
        "                    label_found = True\n",
        "                    break\n",
        "            if not label_found:\n",
        "                updated_labels.append(\"unknown\")  # Default to \"unknown\" if no match is found\n",
        "\n",
        "        # Update the labels in the DataFrame\n",
        "        self.audio_labels['label'] = updated_labels\n",
        "\n",
        "\n",
        "\n",
        "    def _oversample_data(self):\n",
        "        # Convert audio_files (features) and labels to numpy arrays\n",
        "        features = np.array(self.audio_files)\n",
        "        labels = np.array(self.audio_labels['label'])\n",
        "\n",
        "        # Initialize SMOTE\n",
        "        smote = SMOTE(random_state=42)\n",
        "\n",
        "        # Apply SMOTE to oversample the minority classes\n",
        "        features_resampled, labels_resampled = smote.fit_resample(features, labels)\n",
        "\n",
        "        # Update the dataset with the resampled data\n",
        "        self.audio_files = features_resampled.tolist()  # Convert back to list\n",
        "        self.audio_labels = pd.DataFrame({'label': labels_resampled})  # Update labels\n",
        "\n",
        "\n",
        "\n",
        "    def min_max_scale(tensor, min_val=0.0, max_val=1.0):\n",
        "        \"\"\"\n",
        "        Scales a tensor to the range [min_val, max_val].\n",
        "        \"\"\"\n",
        "        tensor_min = tensor.min()\n",
        "        tensor_max = tensor.max()\n",
        "        scaled_tensor = (tensor - tensor_min) / (tensor_max - tensor_min)  # Scale to [0, 1]\n",
        "        scaled_tensor = scaled_tensor * (max_val - min_val) + min_val     # Scale to [min_val, max_val]\n",
        "        return scaled_tensor\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        dml_device = torch_directml.device()\n",
        "\n",
        "\n",
        "        audio_path = self.audio_labels.iloc[idx, 0]\n",
        "        audio, sample_rate = torchaudio.load(audio_path)\n",
        "        audio = audio.to(dml_device)\n",
        "        # Apply transformations if any\n",
        "        label = self.audio_labels.iloc[idx, 1]\n",
        "\n",
        "        if self.transform:\n",
        "            audio = self.transform(audio, sample_rate)\n",
        "\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        # Apply min-max scaling to normalize the spectrogram\n",
        "        audio = min_max_scale(audio, min_val=0.0, max_val=1.0)\n",
        "\n",
        "        audio = T._transforms.ToTensor()(audio)  # Convert audio to tensor\n",
        "        label = T._transforms.ToTensor()(label)  # Convert label to tensor\n",
        "\n",
        "        return audio, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94850bd8",
      "metadata": {
        "id": "94850bd8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "\n",
        "def stratified_split_data(root_dir, train_ratio=0.8):\n",
        "    \"\"\"\n",
        "    Splits audio data into stratified training and testing sets.\n",
        "\n",
        "    Args:\n",
        "        root_dir (str): The root directory containing the audio subdirectories (one per label).\n",
        "        train_ratio (float): The ratio of data to use for training (e.g., 0.8 for 80%).\n",
        "\n",
        "    Creates:\n",
        "        - A \"training\" folder with stratified data.\n",
        "        - A \"test\" folder with stratified data.\n",
        "    \"\"\"\n",
        "    train_dir = os.path.join(root_dir, \"training\")\n",
        "    test_dir = os.path.join(root_dir, \"test\")\n",
        "\n",
        "    # Create train and test directories if they don't exist\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "    total_files = 0\n",
        "\n",
        "    for label in os.listdir(root_dir):\n",
        "        label_dir = os.path.join(root_dir, label)\n",
        "        if os.path.isdir(label_dir) and label not in [\"training\", \"test\"]:  # Skip training and test dirs\n",
        "            files = [f for f in os.listdir(label_dir) if os.path.isfile(os.path.join(label_dir, f))]\n",
        "            random.shuffle(files)  # Shuffle files to ensure randomness\n",
        "\n",
        "            split_index = int(len(files) * train_ratio)\n",
        "            train_files = files[:split_index]\n",
        "            test_files = files[split_index:]\n",
        "\n",
        "            # Create subdirectories for the label in training and test folders\n",
        "            train_label_dir = os.path.join(train_dir, label)\n",
        "            test_label_dir = os.path.join(test_dir, label)\n",
        "            os.makedirs(train_label_dir, exist_ok=True)\n",
        "            os.makedirs(test_label_dir, exist_ok=True)\n",
        "\n",
        "            # Move files to training folder\n",
        "            for file in train_files:\n",
        "                src = os.path.join(label_dir, file)\n",
        "                dst = os.path.join(train_label_dir, file)\n",
        "                shutil.move(src, dst)\n",
        "\n",
        "            # Move files to test folder\n",
        "            for file in test_files:\n",
        "                src = os.path.join(label_dir, file)\n",
        "                dst = os.path.join(test_label_dir, file)\n",
        "                shutil.move(src, dst)\n",
        "\n",
        "            # Update total file count\n",
        "            total_files += len(train_files) + len(test_files)\n",
        "\n",
        "    print(f\"Data splitting completed. Total files: {total_files}\")\n",
        "    if total_files != 9405:\n",
        "        print(f\"Warning: Total file count ({total_files}) does not match the expected count (9405).\")\n",
        "\n",
        "\n",
        "def display_file_distribution(root_dir):\n",
        "    \"\"\"\n",
        "    Displays the file distribution between training, test, and other subdirectories.\n",
        "\n",
        "    Args:\n",
        "        root_dir (str): The root directory containing the 'training' and 'test' subdirectories.\n",
        "    \"\"\"\n",
        "    train_dir = os.path.join(root_dir, \"training\")\n",
        "    test_dir = os.path.join(root_dir, \"test\")\n",
        "\n",
        "    print(\"File Distribution:\")\n",
        "    print(\"===================\")\n",
        "\n",
        "    # Display training and test set distributions\n",
        "    for dataset_type, dataset_dir in [(\"Training\", train_dir), (\"Test\", test_dir)]:\n",
        "        print(f\"\\n{dataset_type} Set:\")\n",
        "        total_files = 0\n",
        "        for label in os.listdir(dataset_dir):\n",
        "            label_dir = os.path.join(dataset_dir, label)\n",
        "            if os.path.isdir(label_dir):\n",
        "                num_files = len(os.listdir(label_dir))\n",
        "                total_files += num_files\n",
        "                print(f\"  {label}: {num_files} files\")\n",
        "        print(f\"Total {dataset_type.lower()} files: {total_files}\")\n",
        "\n",
        "    # Display files in other subdirectories\n",
        "    print(\"\\nOther Subdirectories:\")\n",
        "    total_other_files = 0\n",
        "    for label in os.listdir(root_dir):\n",
        "        label_dir = os.path.join(root_dir, label)\n",
        "        if os.path.isdir(label_dir) and label not in [\"training\", \"test\"]:  # Exclude training and test folders\n",
        "            num_files = len(os.listdir(label_dir))\n",
        "            total_other_files += num_files\n",
        "            print(f\"  {label}: {num_files} files\")\n",
        "    print(f\"Total files in other subdirectories: {total_other_files}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05cd921c",
      "metadata": {
        "id": "05cd921c",
        "outputId": "caa72ff7-e5d6-4d77-dc4c-406af9e8a2ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data splitting completed. Total files: 0\n",
            "Warning: Total file count (0) does not match the expected count (9405).\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "root_dir = \"D:/Vehicle_CNN_Workspace/Audio_Files\"  # Your original directory\n",
        "stratified_split_data(root_dir, train_ratio=0.8)  # Split data into training and testing sets\n",
        "# display_file_distribution(root_dir)  # Display file distribution between training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea6b2b73",
      "metadata": {
        "id": "ea6b2b73"
      },
      "outputs": [],
      "source": [
        "# Define the paths to the training and test directories\n",
        "train_dir = os.path.join(root_dir, \"training\")\n",
        "test_dir = os.path.join(root_dir, \"test\")\n",
        "\n",
        "# Initialize the CustomAudioDataset for training and test data\n",
        "train_dataset = CustomAudioDataset(root_dir=train_dir)\n",
        "test_dataset = CustomAudioDataset(root_dir=test_dir)\n",
        "\n",
        "# Create DataLoaders for training and test datasets\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}